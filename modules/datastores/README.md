# gs-rest-api-datastores
A data store contains vector format spatial data. It can be a file (such as a shapefile), a database (such as PostGIS), or a server (such as a remote Web Feature Service).

This Python package is automatically generated by the [Swagger Codegen](https://github.com/swagger-api/swagger-codegen) project:

- API version: 1.0.0
- Package version: 1.0.0
- Build package: io.swagger.codegen.v3.generators.python.PythonClientCodegen
For more information, please visit [http://geoserver.org/comm/](http://geoserver.org/comm/)

## Requirements.

Python 2.7 and 3.4+

## Installation & Usage
### pip install

If the python package is hosted on Github, you can install directly from Github

```sh
pip install git+https://github.com/ausseabed/geoserver-rest-client.git#subdirectory=modules/datastores\&ignore=.git
```
(you may need to run `pip` with root permission: `sudo pip install git+https://github.com/ausseabed/geoserver-rest-client.git#subdirectory=modules/datastores\&ignore=.git`)

Then import the package:
```python
import gs_rest_api_datastores 
```

### Setuptools

Install via [Setuptools](http://pypi.python.org/pypi/setuptools).

```sh
python setup.py install --user
```
(or `sudo python setup.py install` to install the package for all users)

Then import the package:
```python
import gs_rest_api_datastores
```

## Getting Started

Please follow the [installation procedure](#installation--usage) and then run the following:

```python
from __future__ import print_function
import time
import gs_rest_api_datastores
from gs_rest_api_datastores.rest import ApiException
from pprint import pprint

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.
store_name = 'store_name_example' # str | The name of the App-Schema store

try:
    # Cleans all MongoDB internal stores Schemas for an App-Schema store.
    api_instance.clean_all_mongo_schemas(workspace_name, store_name)
except ApiException as e:
    print("Exception when calling DefaultApi->clean_all_mongo_schemas: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.
store_name = 'store_name_example' # str | The name of the App-Schema store
internal_store_id = 'internal_store_id_example' # str | The store ID for the internal MongoDB store as specified on App-Schema Mappings.

try:
    # Cleans a MongoDB internal store Schemas for an App-Schema store.
    api_instance.clean_mongo_schema(workspace_name, store_name, internal_store_id)
except ApiException as e:
    print("Exception when calling DefaultApi->clean_mongo_schema: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))

try:
    api_instance.delete_data_store_upload()
except ApiException as e:
    print("Exception when calling DefaultApi->delete_data_store_upload: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data store.
store_name = 'store_name_example' # str | The name of the data store to delete.
recurse = true # bool | The recurse controls recursive deletion. When set to true all resources contained in the store are also removed. The default value is \"false\". (optional)

try:
    # Delete data store
    api_instance.delete_datastore(workspace_name, store_name, recurse=recurse)
except ApiException as e:
    print("Exception when calling DefaultApi->delete_datastore: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))

try:
    api_instance.deletedatastores()
except ApiException as e:
    print("Exception when calling DefaultApi->deletedatastores: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data store.
store_name = 'store_name_example' # str | The name of the data store to retrieve.
quiet_on_not_found = true # bool | The quietOnNotFound parameter avoids logging an exception when the data store is not present. Note that 404 status code will still be returned. (optional)

try:
    # Retrieve a particular data store from a workspace
    api_response = api_instance.get_data_store(workspace_name, store_name, quiet_on_not_found=quiet_on_not_found)
    pprint(api_response)
except ApiException as e:
    print("Exception when calling DefaultApi->get_data_store: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data store.
store_name = 'store_name_example' # str | The name of the store to be retrieved
method = 'method_example' # str | The upload method. Can be \"url\", \"file\", \"external\". Unused for GET
format = 'format_example' # str | The type of source data store (e.g., \"shp\"). Unused for GET

try:
    api_instance.get_data_store_upload(workspace_name, store_name, method, format)
except ApiException as e:
    print("Exception when calling DefaultApi->get_data_store_upload: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.

try:
    # Get a list of data stores
    api_response = api_instance.get_datastores(workspace_name)
    pprint(api_response)
except ApiException as e:
    print("Exception when calling DefaultApi->get_datastores: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))

try:
    api_instance.post_data_store_upload()
except ApiException as e:
    print("Exception when calling DefaultApi->post_data_store_upload: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))

try:
    api_instance.post_datastore()
except ApiException as e:
    print("Exception when calling DefaultApi->post_datastore: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
body = gs_rest_api_datastores.Datastore() # Datastore | The data store body information to upload.

The contents of the connection parameters will differ depending on the type of data store being added.

- GeoPackage

  Examples:
  - application/xml:

    ```
    <dataStore>
      <name>nyc</name>
      <connectionParameters>
        <database>file:///path/to/nyc.gpkg</database>
        <dbtype>geopkg</dbtype>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "name": "nyc",
        "connectionParameters": {
          "entry": [
            {"@key":"database","$":"file:///path/to/nyc.gpkg"},
            {"@key":"dbtype","$":"geopkg"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Primary key metadata table | The optional table containing primary key structure and sequence associations. Can be expressed as 'schema.name' or just 'name' | user | String | False | ` ` |
  | Callback factory | Name of JDBCReaderCallbackFactory to enable on the data store | user | String | False | ` ` |
  | Evictor tests per run | number of connections checked by the idle connection evictor for each of its runs (defaults to 3) | user | Integer | False | `3` |
  | database | Database | user | File | True | ` ` |
  | Batch insert size | Number of records inserted in the same batch (default, 1). For optimal performance, set to 100. | user | Integer | False | `1` |
  | fetch size | number of records read with each iteraction with the dbms | user | Integer | False | `1000` |
  | Connection timeout | number of seconds the connection pool will wait before timing out attempting to get a new connection (default, 20 seconds) | user | Integer | False | `20` |
  | namespace | Namespace prefix | user | String | False | ` ` |
  | max connections | maximum number of open connections | user | Integer | False | `10` |
  | Test while idle | Periodically test the connections are still valid also while idle in the pool | user | Boolean | False | `True` |
  | Max connection idle time | number of seconds a connection needs to stay idle for the evictor to consider closing it | user | Integer | False | `300` |
  | Session startup SQL | SQL statement executed when the connection is grabbed from the pool | user | String | False | ` ` |
  | validate connections | check connection is alive before using it | user | Boolean | False | `True` |
  | dbtype | Type | program | String | True | `geopkg` |
  | passwd | password used to login | user | String | False | ` ` |
  | Expose primary keys | Expose primary key columns as attributes of the feature type | user | Boolean | False | `False` |
  | min connections | minimum number of pooled connection | user | Integer | False | `1` |
  | Evictor run periodicity | number of seconds between idle object evitor runs (default, 300 seconds) | user | Integer | False | `300` |
  | Session close-up SQL | SQL statement executed when the connection is released to the pool | user | String | False | ` ` |
  | user | user name to login as | user | String | False | ` ` |

- PostGIS

  Examples:
  - application/xml:

    ```
    <dataStore>
      <name>nyc</name>
      <connectionParameters>
        <host>localhost</host>
        <port>5432</port>
        <database>nyc</database>
        <user>bob</user>
        <passwd>postgres</passwd>
        <dbtype>postgis</dbtype>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "name": "nyc",
        "connectionParameters": {
          "entry": [
            {"@key":"host","$":"localhost"},
            {"@key":"port","$":"5432"},
            {"@key":"database","$":"nyc"},
            {"@key":"user","$":"bob"},
            {"@key":"passwd","$":"postgres"},
            {"@key":"dbtype","$":"postgis"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Connection timeout | number of seconds the connection pool will wait before timing out attempting to get a new connection (default, 20 seconds) | user | Integer | False | `20` |
  | validate connections | check connection is alive before using it | user | Boolean | False | `True` |
  | port | Port | user | Integer | True | `5432` |
  | Primary key metadata table | The optional table containing primary key structure and sequence associations. Can be expressed as 'schema.name' or just 'name' | user | String | False | ` ` |
  | Support on the fly geometry simplification | When enabled, operations such as map rendering will pass a hint that will enable the usage of ST_Simplify | user | Boolean | False | `True` |
  | create database | Creates the database if it does not exist yet | advanced | Boolean | False | `False` |
  | create database params | Extra specifications appeneded to the CREATE DATABASE command | advanced | String | False | `` |
  | dbtype | Type | program | String | True | `postgis` |
  | Batch insert size | Number of records inserted in the same batch (default, 1). For optimal performance, set to 100. | user | Integer | False | `1` |
  | namespace | Namespace prefix | user | String | False | ` ` |
  | Max connection idle time | number of seconds a connection needs to stay idle for the evictor to consider closing it | user | Integer | False | `300` |
  | Session startup SQL | SQL statement executed when the connection is grabbed from the pool | user | String | False | ` ` |
  | Expose primary keys | Expose primary key columns as attributes of the feature type | user | Boolean | False | `False` |
  | min connections | minimum number of pooled connection | user | Integer | False | `1` |
  | Max open prepared statements | Maximum number of prepared statements kept open and cached for each connection in the pool. Set to 0 to have unbounded caching, to -1 to disable caching | user | Integer | False | `50` |
  | Callback factory | Name of JDBCReaderCallbackFactory to enable on the data store | user | String | False | ` ` |
  | passwd | password used to login | user | String | False | ` ` |
  | encode functions | set to true to have a set of filter functions be translated directly in SQL. Due to differences in the type systems the result might not be the same as evaluating them in memory, including the SQL failing with errors while the in memory version works fine. However this allows to push more of the filter into the database, increasing performance.the postgis table. | advanced | Boolean | False | `False` |
  | host | Host | user | String | True | `localhost` |
  | Evictor tests per run | number of connections checked by the idle connection evictor for each of its runs (defaults to 3) | user | Integer | False | `3` |
  | Loose bbox | Perform only primary filter on bbox | user | Boolean | False | `True` |
  | Evictor run periodicity | number of seconds between idle object evitor runs (default, 300 seconds) | user | Integer | False | `300` |
  | Estimated extends | Use the spatial index information to quickly get an estimate of the data bounds | user | Boolean | False | `True` |
  | database | Database | user | String | False | ` ` |
  | fetch size | number of records read with each iteraction with the dbms | user | Integer | False | `1000` |
  | Test while idle | Periodically test the connections are still valid also while idle in the pool | user | Boolean | False | `True` |
  | max connections | maximum number of open connections | user | Integer | False | `10` |
  | preparedStatements | Use prepared statements | user | Boolean | False | `False` |
  | Session close-up SQL | SQL statement executed when the connection is released to the pool | user | String | False | ` ` |
  | schema | Schema | user | String | False | `public` |
  | user | user name to login as | user | String | True | ` ` |

- Shapefile

  Examples:
  - application/xml:

    ```
    <dataStore>
      <name>nyc</name>
      <connectionParameters>
        <url>file:/path/to/nyc.shp</database>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "name": "nyc",
        "connectionParameters": {
          "entry": [
            {"@key":"url","$":"file:/path/to/nyc.shp"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | cache and reuse memory maps | only memory map a file one, then cache and reuse the map | advanced | Boolean | False | `True` |
  | namespace | uri to a the namespace | advanced | URI | False | ` ` |
  | filetype | Discriminator for directory stores | program | String | False | `shapefile` |
  | charset | character used to decode strings from the DBF file | advanced | Charset | False | `ISO-8859-1` |
  | create spatial index | enable/disable the automatic creation of spatial index | advanced | Boolean | False | `True` |
  | fstype | Enable using a setting of 'shape'. | advanced | String | False | `shape` |
  | url | url to a .shp file | user | URL | True | ` ` |
  | enable spatial index | enable/disable the use of spatial index for local shapefiles | advanced | Boolean | False | `True` |
  | memory mapped buffer | enable/disable the use of memory-mapped io | advanced | Boolean | False | `False` |
  | timezone | time zone used to read dates from the DBF file | advanced | TimeZone | False | `Pacific Standard Time` |

- Directory of spatial files (shapefiles)

  Examples:
  - application/xml:

    ```
    <dataStore>
      <name>nyc</name>
      <connectionParameters>
        <url>file:/path/to/directory</database>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "name": "nyc",
        "connectionParameters": {
          "entry": [
            {"@key":"url","$":"file:/path/to/directory"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | cache and reuse memory maps | only memory map a file one, then cache and reuse the map | advanced | Boolean | False | `True` |
  | namespace | uri to a the namespace | advanced | URI | False | ` ` |
  | filetype | Discriminator for directory stores | program | String | False | `shapefile` |
  | charset | character used to decode strings from the DBF file | advanced | Charset | False | `ISO-8859-1` |
  | create spatial index | enable/disable the automatic creation of spatial index | advanced | Boolean | False | `True` |
  | fstype | Enable using a setting of 'shape'. | advanced | String | False | `shape` |
  | url | url to a .shp file | user | URL | True | ` ` |
  | enable spatial index | enable/disable the use of spatial index for local shapefiles | advanced | Boolean | False | `True` |
  | memory mapped buffer | enable/disable the use of memory-mapped io | advanced | Boolean | False | `False` |
  | timezone | time zone used to read dates from the DBF file | advanced | TimeZone | False | `Pacific Standard Time` |


- Web Feature Service

  Examples:
  - application/xml:

    ```
    <dataStore>
      <name>nyc</name>
      <connectionParameters>
        <GET_CAPABILITIES_URL>http://localhost:8080/geoserver/wfs?request=GetCapabilities</GET_CAPABILITIES_URL>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "name": "nyc",
        "connectionParameters": {
          "entry": [
            {"@key":"GET_CAPABILITIES_URL","$":"http://localhost:8080/geoserver/wfs?request=GetCapabilities"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Protocol | Sets a preference for the HTTP protocol to use when requesting WFS functionality. Set this value to Boolean.TRUE for POST, Boolean.FALSE for GET or NULL for AUTO | user | Boolean | False | ` ` |
  | WFS GetCapabilities URL | Represents a URL to the getCapabilities document or a server instance. | user | URL | False | ` ` |
  | Buffer Size | This allows the user to specify a buffer size in features. This param has a default value of 10 features. | user | Integer | False | `10` |
  | Filter compliance | Level of compliance to WFS specification (0-low,1-medium,2-high) | user | Integer | False | ` ` |
  | EntityResolver | Sets the entity resolver used to expand XML entities | program | EntityResolver | False | `org.geotools.xml.PreventLocalEntityResolver@75e98519` |
  | Time-out | This allows the user to specify a timeout in milliseconds. This param has a default value of 3000ms. | user | Integer | False | `3000` |
  | GmlComplianceLevel | Optional OGC GML compliance level required. | user | Integer | False | `0` |
  | Lenient | Indicates that datastore should do its best to create features from the provided data even if it does not accurately match the schema.  Errors will be logged but the parsing will continue if this is true.  Default is false | user | Boolean | False | `False` |
  | Password | This allows the user to specify a username. This param should not be used without the USERNAME param. | user | String | False | ` ` |
  | Use Default SRS | Use always the declared DefaultSRS for requests and reproject locally if necessary | advanced | Boolean | False | `False` |
  | Namespace | Override the original WFS type name namespaces | advanced | String | False | ` ` |
  | Username | This allows the user to specify a username. This param should not be used without the PASSWORD param. | user | String | False | ` ` |
  | Axis Order Filter | Indicates axis order used by the remote WFS server for filters. It applies only to WFS 1.x.0 servers. Default is the same as AXIS_ORDER | advanced | String | False | ` ` |
  | GmlCompatibleTypeNames | Use Gml Compatible TypeNames (replace : by _). | user | Boolean | False | `False` |
  | Maximum features | Positive integer used as a hard limit for the amount of Features to retrieve for each FeatureType. A value of zero or not providing this parameter means no limit. | user | Integer | False | `0` |
  | Axis Order | Indicates axis order used by the remote WFS server in result coordinates. It applies only to WFS 1.x.0 servers. Default is Compliant | advanced | String | False | `Compliant` |
  | WFS Strategy | Override wfs stragegy with either cubwerx, ionic, mapserver, geoserver, strict, nonstrict or arcgis strategy. | user | String | False | `auto` |
  | Try GZIP | Indicates that datastore should use gzip to transfer data if the server supports it. Default is true | user | Boolean | False | `True` |
  | Encoding | This allows the user to specify the character encoding of the XML-Requests sent to the Server. Defaults to UTF-8 | user | String | False | `UTF-8` |
  | Outputformat | This allows the user to specify an outputFormat, different from the default one. | advanced | String | False | ` ` |

workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.

try:
    # Create a new data store
    api_response = api_instance.post_datastores(body, workspace_name)
    pprint(api_response)
except ApiException as e:
    print("Exception when calling DefaultApi->post_datastores: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the coverage stores.
store_name = 'store_name_example' # str | The name of the store to be retrieved
method = 'method_example' # str | The upload method. Can be \"url\", \"file\", \"external\". \"file\" uploads a file from a local source. The body of the request is the file itself. \"url\" uploads a file from an remote source. The body of the request is a URL pointing to the file to upload. This URL must be visible from the server. \"external\" uses an existing file on the server. The body of the request is the absolute path to the existing file.
format = 'format_example' # str | The type of source data store (e.g., \"shp\").
upfile = 'upfile_example' # str |  (optional)
configure = 'configure_example' # str | The configure parameter controls if a coverage/layer are configured upon file upload, in addition to creating the store. It can have a value of \"none\" to avoid configuring coverages. (optional)
target = 'target_example' # str | The type of target data store (e.g., \"shp\"). Same as format if not provided. (optional)
update = 'update_example' # str | The update mode. If \"overwrite\", will overwrite existing data. Otherwise, will append to existing data. (optional)
charset = 'charset_example' # str | The character set of the data. (optional)
filename = 'filename_example' # str | The filename parameter specifies the target file name for the file to be uploaded. This is important to avoid clashes with existing files. (optional)

try:
    # Uploads files to the data store, creating it if necessary
    api_instance.put_data_store_upload(workspace_name, store_name, method, format, upfile=upfile, configure=configure, target=target, update=update, charset=charset, filename=filename)
except ApiException as e:
    print("Exception when calling DefaultApi->put_data_store_upload: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
body = gs_rest_api_datastores.Datastore() # Datastore | The updated data store definition.
For a PUT, only values which should be changed need to be included. The connectionParameters map counts as a single value, 
so if you change it all preexisting connection parameters will be overwritten.

The contents of the connection parameters will differ depending on the type of data store being added.

- GeoPackage

  Examples:
  - application/xml:

    ```
    <dataStore>
      <description>A data store</description>
      <enabled>true</enabled>
      <__default>true</__default>
      <connectionParameters>
        <database>file:///path/to/nyc.gpkg</database>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "description": "A data store",
        "enabled": "true",
        "_default": "true",
        "connectionParameters": {
          "entry": [
            {"@key":"database","$":"file:///path/to/nyc.gpkg"},
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Primary key metadata table | The optional table containing primary key structure and sequence associations. Can be expressed as 'schema.name' or just 'name' | user | String | False | ` ` |
  | Callback factory | Name of JDBCReaderCallbackFactory to enable on the data store | user | String | False | ` ` |
  | Evictor tests per run | number of connections checked by the idle connection evictor for each of its runs (defaults to 3) | user | Integer | False | `3` |
  | database | Database | user | File | True | ` ` |
  | Batch insert size | Number of records inserted in the same batch (default, 1). For optimal performance, set to 100. | user | Integer | False | `1` |
  | fetch size | number of records read with each iteraction with the dbms | user | Integer | False | `1000` |
  | Connection timeout | number of seconds the connection pool will wait before timing out attempting to get a new connection (default, 20 seconds) | user | Integer | False | `20` |
  | namespace | Namespace prefix | user | String | False | ` ` |
  | max connections | maximum number of open connections | user | Integer | False | `10` |
  | Test while idle | Periodically test the connections are still valid also while idle in the pool | user | Boolean | False | `True` |
  | Max connection idle time | number of seconds a connection needs to stay idle for the evictor to consider closing it | user | Integer | False | `300` |
  | Session startup SQL | SQL statement executed when the connection is grabbed from the pool | user | String | False | ` ` |
  | validate connections | check connection is alive before using it | user | Boolean | False | `True` |
  | dbtype | Type | program | String | True | `geopkg` |
  | passwd | password used to login | user | String | False | ` ` |
  | Expose primary keys | Expose primary key columns as attributes of the feature type | user | Boolean | False | `False` |
  | min connections | minimum number of pooled connection | user | Integer | False | `1` |
  | Evictor run periodicity | number of seconds between idle object evitor runs (default, 300 seconds) | user | Integer | False | `300` |
  | Session close-up SQL | SQL statement executed when the connection is released to the pool | user | String | False | ` ` |
  | user | user name to login as | user | String | False | ` ` |

- PostGIS

  Examples:
  - application/xml:

    ```
    <dataStore>
      <description>A data store</description>
      <enabled>true</enabled>
      <__default>true</__default>
      <connectionParameters>
        <host>localhost</host>
        <port>5432</port>
        <database>nyc</database>
        <user>bob</user>
        <passwd>postgres</passwd>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "description": "A data store",
        "enabled": "true",
        "_default": "true",
        "connectionParameters": {
          "entry": [
            {"@key":"host","$":"localhost"},
            {"@key":"port","$":"5432"},
            {"@key":"database","$":"nyc"},
            {"@key":"user","$":"bob"},
            {"@key":"passwd","$":"postgres"},
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Connection timeout | number of seconds the connection pool will wait before timing out attempting to get a new connection (default, 20 seconds) | user | Integer | False | `20` |
  | validate connections | check connection is alive before using it | user | Boolean | False | `True` |
  | port | Port | user | Integer | True | `5432` |
  | Primary key metadata table | The optional table containing primary key structure and sequence associations. Can be expressed as 'schema.name' or just 'name' | user | String | False | ` ` |
  | Support on the fly geometry simplification | When enabled, operations such as map rendering will pass a hint that will enable the usage of ST_Simplify | user | Boolean | False | `True` |
  | create database | Creates the database if it does not exist yet | advanced | Boolean | False | `False` |
  | create database params | Extra specifications appeneded to the CREATE DATABASE command | advanced | String | False | `` |
  | dbtype | Type | program | String | True | `postgis` |
  | Batch insert size | Number of records inserted in the same batch (default, 1). For optimal performance, set to 100. | user | Integer | False | `1` |
  | namespace | Namespace prefix | user | String | False | ` ` |
  | Max connection idle time | number of seconds a connection needs to stay idle for the evictor to consider closing it | user | Integer | False | `300` |
  | Session startup SQL | SQL statement executed when the connection is grabbed from the pool | user | String | False | ` ` |
  | Expose primary keys | Expose primary key columns as attributes of the feature type | user | Boolean | False | `False` |
  | min connections | minimum number of pooled connection | user | Integer | False | `1` |
  | Max open prepared statements | Maximum number of prepared statements kept open and cached for each connection in the pool. Set to 0 to have unbounded caching, to -1 to disable caching | user | Integer | False | `50` |
  | Callback factory | Name of JDBCReaderCallbackFactory to enable on the data store | user | String | False | ` ` |
  | passwd | password used to login | user | String | False | ` ` |
  | encode functions | set to true to have a set of filter functions be translated directly in SQL. Due to differences in the type systems the result might not be the same as evaluating them in memory, including the SQL failing with errors while the in memory version works fine. However this allows to push more of the filter into the database, increasing performance.the postgis table. | advanced | Boolean | False | `False` |
  | host | Host | user | String | True | `localhost` |
  | Evictor tests per run | number of connections checked by the idle connection evictor for each of its runs (defaults to 3) | user | Integer | False | `3` |
  | Loose bbox | Perform only primary filter on bbox | user | Boolean | False | `True` |
  | Evictor run periodicity | number of seconds between idle object evitor runs (default, 300 seconds) | user | Integer | False | `300` |
  | Estimated extends | Use the spatial index information to quickly get an estimate of the data bounds | user | Boolean | False | `True` |
  | database | Database | user | String | False | ` ` |
  | fetch size | number of records read with each iteraction with the dbms | user | Integer | False | `1000` |
  | Test while idle | Periodically test the connections are still valid also while idle in the pool | user | Boolean | False | `True` |
  | max connections | maximum number of open connections | user | Integer | False | `10` |
  | preparedStatements | Use prepared statements | user | Boolean | False | `False` |
  | Session close-up SQL | SQL statement executed when the connection is released to the pool | user | String | False | ` ` |
  | schema | Schema | user | String | False | `public` |
  | user | user name to login as | user | String | True | ` ` |

- Shapefile

  Examples:
  - application/xml:

    ```
    <dataStore>
      <description>A data store</description>
      <enabled>true</enabled>
      <__default>true</__default>
      <connectionParameters>
        <url>file:/path/to/nyc.shp</database>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "description": "A data store",
        "enabled": "true",
        "_default": "true",
        "connectionParameters": {
          "entry": [
            {"@key":"url","$":"file:/path/to/nyc.shp"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | cache and reuse memory maps | only memory map a file one, then cache and reuse the map | advanced | Boolean | False | `True` |
  | namespace | uri to a the namespace | advanced | URI | False | ` ` |
  | filetype | Discriminator for directory stores | program | String | False | `shapefile` |
  | charset | character used to decode strings from the DBF file | advanced | Charset | False | `ISO-8859-1` |
  | create spatial index | enable/disable the automatic creation of spatial index | advanced | Boolean | False | `True` |
  | fstype | Enable using a setting of 'shape'. | advanced | String | False | `shape` |
  | url | url to a .shp file | user | URL | True | ` ` |
  | enable spatial index | enable/disable the use of spatial index for local shapefiles | advanced | Boolean | False | `True` |
  | memory mapped buffer | enable/disable the use of memory-mapped io | advanced | Boolean | False | `False` |
  | timezone | time zone used to read dates from the DBF file | advanced | TimeZone | False | `Pacific Standard Time` |

- Directory of spatial files (shapefiles)

  Examples:
  - application/xml:

    ```
    <dataStore>
      <description>A data store</description>
      <enabled>true</enabled>
      <__default>true</__default>
      <connectionParameters>
        <url>file:/path/to/directory</database>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "description": "A data store",
        "enabled": "true",
        "_default": "true",
        "connectionParameters": {
          "entry": [
            {"@key":"url","$":"file:/path/to/directory"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | cache and reuse memory maps | only memory map a file one, then cache and reuse the map | advanced | Boolean | False | `True` |
  | namespace | uri to a the namespace | advanced | URI | False | ` ` |
  | filetype | Discriminator for directory stores | program | String | False | `shapefile` |
  | charset | character used to decode strings from the DBF file | advanced | Charset | False | `ISO-8859-1` |
  | create spatial index | enable/disable the automatic creation of spatial index | advanced | Boolean | False | `True` |
  | fstype | Enable using a setting of 'shape'. | advanced | String | False | `shape` |
  | url | url to a .shp file | user | URL | True | ` ` |
  | enable spatial index | enable/disable the use of spatial index for local shapefiles | advanced | Boolean | False | `True` |
  | memory mapped buffer | enable/disable the use of memory-mapped io | advanced | Boolean | False | `False` |
  | timezone | time zone used to read dates from the DBF file | advanced | TimeZone | False | `Pacific Standard Time` |


- Web Feature Service

  Examples:
  - application/xml:

    ```
    <dataStore>
      <description>A data store</description>
      <enabled>true</enabled>
      <__default>true</__default>
      <connectionParameters>
        <GET_CAPABILITIES_URL>http://localhost:8080/geoserver/wfs?request=GetCapabilities</GET_CAPABILITIES_URL>
      </connectionParameters>
    </dataStore>
    ```

  - application/json:

    ```
    {
      "dataStore": {
        "description": "A data store",
        "enabled": "true",
        "_default": "true",
        "connectionParameters": {
          "entry": [
            {"@key":"GET_CAPABILITIES_URL","$":"http://localhost:8080/geoserver/wfs?request=GetCapabilities"}
          ]
        }
      }
    }
    ```

  Connection Parameters:

  | key | description | level | type | required | default |
  | --- | ----------- | ----- | ---- | -------- | ------- |
  | Protocol | Sets a preference for the HTTP protocol to use when requesting WFS functionality. Set this value to Boolean.TRUE for POST, Boolean.FALSE for GET or NULL for AUTO | user | Boolean | False | ` ` |
  | WFS GetCapabilities URL | Represents a URL to the getCapabilities document or a server instance. | user | URL | False | ` ` |
  | Buffer Size | This allows the user to specify a buffer size in features. This param has a default value of 10 features. | user | Integer | False | `10` |
  | Filter compliance | Level of compliance to WFS specification (0-low,1-medium,2-high) | user | Integer | False | ` ` |
  | EntityResolver | Sets the entity resolver used to expand XML entities | program | EntityResolver | False | `org.geotools.xml.PreventLocalEntityResolver@75e98519` |
  | Time-out | This allows the user to specify a timeout in milliseconds. This param has a default value of 3000ms. | user | Integer | False | `3000` |
  | GmlComplianceLevel | Optional OGC GML compliance level required. | user | Integer | False | `0` |
  | Lenient | Indicates that datastore should do its best to create features from the provided data even if it does not accurately match the schema.  Errors will be logged but the parsing will continue if this is true.  Default is false | user | Boolean | False | `False` |
  | Password | This allows the user to specify a username. This param should not be used without the USERNAME param. | user | String | False | ` ` |
  | Use Default SRS | Use always the declared DefaultSRS for requests and reproject locally if necessary | advanced | Boolean | False | `False` |
  | Namespace | Override the original WFS type name namespaces | advanced | String | False | ` ` |
  | Username | This allows the user to specify a username. This param should not be used without the PASSWORD param. | user | String | False | ` ` |
  | Axis Order Filter | Indicates axis order used by the remote WFS server for filters. It applies only to WFS 1.x.0 servers. Default is the same as AXIS_ORDER | advanced | String | False | ` ` |
  | GmlCompatibleTypeNames | Use Gml Compatible TypeNames (replace : by _). | user | Boolean | False | `False` |
  | Maximum features | Positive integer used as a hard limit for the amount of Features to retrieve for each FeatureType. A value of zero or not providing this parameter means no limit. | user | Integer | False | `0` |
  | Axis Order | Indicates axis order used by the remote WFS server in result coordinates. It applies only to WFS 1.x.0 servers. Default is Compliant | advanced | String | False | `Compliant` |
  | WFS Strategy | Override wfs stragegy with either cubwerx, ionic, mapserver, geoserver, strict, nonstrict or arcgis strategy. | user | String | False | `auto` |
  | Try GZIP | Indicates that datastore should use gzip to transfer data if the server supports it. Default is true | user | Boolean | False | `True` |
  | Encoding | This allows the user to specify the character encoding of the XML-Requests sent to the Server. Defaults to UTF-8 | user | String | False | `UTF-8` |
  | Outputformat | This allows the user to specify an outputFormat, different from the default one. | advanced | String | False | ` ` |

workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data store.
store_name = 'store_name_example' # str | The name of the data store to modify.

try:
    # Modify a data store.
    api_instance.put_datastore(body, workspace_name, store_name)
except ApiException as e:
    print("Exception when calling DefaultApi->put_datastore: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))

try:
    api_instance.putdatastores()
except ApiException as e:
    print("Exception when calling DefaultApi->putdatastores: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.
store_name = 'store_name_example' # str | The name of the App-Schema store
ids = 'ids_example' # str | Comma separated MongoDB object IDs for use in new generated schema. (optional)
max = 56 # int | Max number of objects for use in new generated schema. (optional)

try:
    # Rebuilds all MongoDB internal stores Schemas for an App-Schema store.
    api_instance.rebuild_all_mongo_schemas(workspace_name, store_name, ids=ids, max=max)
except ApiException as e:
    print("Exception when calling DefaultApi->rebuild_all_mongo_schemas: %s\n" % e)

# create an instance of the API class
api_instance = gs_rest_api_datastores.DefaultApi(gs_rest_api_datastores.ApiClient(configuration))
workspace_name = 'workspace_name_example' # str | The name of the worskpace containing the data stores.
store_name = 'store_name_example' # str | The name of the App-Schema store
internal_store_id = 'internal_store_id_example' # str | The store ID for the internal MongoDB store as specified on App-Schema Mappings.
ids = 'ids_example' # str | Comma separated MongoDB object IDs for use in new generated schema. (optional)
max = 56 # int | Max number of objects for use in new generated schema. (optional)
schema = 'schema_example' # str | Name of schema to re-build. (optional)

try:
    # Rebuilds a MongoDB internal store Schemas for an App-Schema store.
    api_instance.rebuild_mongo_schema(workspace_name, store_name, internal_store_id, ids=ids, max=max, schema=schema)
except ApiException as e:
    print("Exception when calling DefaultApi->rebuild_mongo_schema: %s\n" % e)
```

## Documentation for API Endpoints

All URIs are relative to *//localhost:8080/geoserver/rest*

Class | Method | HTTP request | Description
------------ | ------------- | ------------- | -------------
*DefaultApi* | [**clean_all_mongo_schemas**](docs/DefaultApi.md#clean_all_mongo_schemas) | **POST** /workspaces/{workspaceName}/appschemastores/{storeName}/cleanSchemas | Cleans all MongoDB internal stores Schemas for an App-Schema store.
*DefaultApi* | [**clean_mongo_schema**](docs/DefaultApi.md#clean_mongo_schema) | **POST** /workspaces/{workspaceName}/appschemastores/{storeName}/datastores/{internalStoreId}/cleanSchemas | Cleans a MongoDB internal store Schemas for an App-Schema store.
*DefaultApi* | [**delete_data_store_upload**](docs/DefaultApi.md#delete_data_store_upload) | **DELETE** /workspaces/{workspaceName}/datastores/{storeName}/{method}.{format} | 
*DefaultApi* | [**delete_datastore**](docs/DefaultApi.md#delete_datastore) | **DELETE** /workspaces/{workspaceName}/datastores/{storeName} | Delete data store
*DefaultApi* | [**deletedatastores**](docs/DefaultApi.md#deletedatastores) | **DELETE** /workspaces/{workspaceName}/datastores | 
*DefaultApi* | [**get_data_store**](docs/DefaultApi.md#get_data_store) | **GET** /workspaces/{workspaceName}/datastores/{storeName} | Retrieve a particular data store from a workspace
*DefaultApi* | [**get_data_store_upload**](docs/DefaultApi.md#get_data_store_upload) | **GET** /workspaces/{workspaceName}/datastores/{storeName}/{method}.{format} | 
*DefaultApi* | [**get_datastores**](docs/DefaultApi.md#get_datastores) | **GET** /workspaces/{workspaceName}/datastores | Get a list of data stores
*DefaultApi* | [**post_data_store_upload**](docs/DefaultApi.md#post_data_store_upload) | **POST** /workspaces/{workspaceName}/datastores/{storeName}/{method}.{format} | 
*DefaultApi* | [**post_datastore**](docs/DefaultApi.md#post_datastore) | **POST** /workspaces/{workspaceName}/datastores/{storeName} | 
*DefaultApi* | [**post_datastores**](docs/DefaultApi.md#post_datastores) | **POST** /workspaces/{workspaceName}/datastores | Create a new data store
*DefaultApi* | [**put_data_store_upload**](docs/DefaultApi.md#put_data_store_upload) | **PUT** /workspaces/{workspaceName}/datastores/{storeName}/{method}.{format} | Uploads files to the data store, creating it if necessary
*DefaultApi* | [**put_datastore**](docs/DefaultApi.md#put_datastore) | **PUT** /workspaces/{workspaceName}/datastores/{storeName} | Modify a data store.
*DefaultApi* | [**putdatastores**](docs/DefaultApi.md#putdatastores) | **PUT** /workspaces/{workspaceName}/datastores | 
*DefaultApi* | [**rebuild_all_mongo_schemas**](docs/DefaultApi.md#rebuild_all_mongo_schemas) | **POST** /workspaces/{workspaceName}/appschemastores/{storeName}/rebuildMongoSchemas | Rebuilds all MongoDB internal stores Schemas for an App-Schema store.
*DefaultApi* | [**rebuild_mongo_schema**](docs/DefaultApi.md#rebuild_mongo_schema) | **POST** /workspaces/{workspaceName}/appschemastores/{storeName}/datastores/{internalStoreId}/rebuildMongoSchemas | Rebuilds a MongoDB internal store Schemas for an App-Schema store.

## Documentation For Models

 - [DataStoreResponse](docs/DataStoreResponse.md)
 - [Datastore](docs/Datastore.md)

## Documentation For Authorization

 All endpoints do not require authorization.


## Author

geoserver-users@sourceforge.net
